# From notebook to server in seconds

Modern notebook workflows are messy and complex. Notebooks add value through interactivity and fast action to response times, but taking these to production often requires a full rewrite.

This repo presents a set of tools to take a notebook rich with exprerimentation and convert it into a production-ready ML server in seconds.

The primary idea is that any code used to manipulate the data can be extracted into a production environment by simply tagging it with a class and method name.

The class files are generated automatically, then, a YAML file specifies the order in which the classes are executed and a flask server is deployed to execute those workflows automatically.

`notebook.ipynb` is an example of a test notebook. The cells in the notebook are tagged with `%%add_to TitanicModelTrainer train_model`. Those cells are then run in order when training the model. Similarly, there are cells for data pre-processing and so on.

`workflows.yaml` defines two workflows, one for inference, which is just preprocessing and model serving and one for training, which is preprocessing and training.

Then, to test this, run:
```
docker compose up
curl -X POST -F "file=@train.csv" http://localhost:5001/training
curl -X POST -F "file=@test.csv" http://localhost:5001/inference  
```

The rest of the code is just library/support code. Most of the logic is autogenerated. Long term, this can also easily include tests, arbitrarly many different workflows, composite models and some really clever CICD pipelining.